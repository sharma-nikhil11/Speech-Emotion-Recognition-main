# Speech-Emotion-Recognition-main

## Project Overview
This project focuses on the development of a Speech Emotion Recognition (SER) system that identifies and classifies emotions in spoken language. The system uses Recurrent Neural Networks (RNN) and Long Short-Term Memory (LSTM) networks to process and analyze speech data, enabling real-time emotion detection based on pitch, tone, and rhythm.
The model aims to accurately identify various emotions such as happiness, sadness, surprise, etc., and has applications in areas like virtual assistants, customer service, and mental health monitoring.


## Features
#### Emotion Detection: Classifies emotions in speech through audio feature extraction.
#### Real-time Processing: Detects emotions in real-time for dynamic applications.
#### Deep Learning Model: Utilizes advanced RNN/LSTM architectures for higher accuracy in emotion detection.
#### Data Preprocessing: Includes normalization, feature extraction, and data augmentation techniques.
#### High Performance: Achieves an accuracy of approximately 85% on test data.


## Dataset
For the Speech Emotion Recognition (SER) model, the dataset used is the RAVDESS (Ryerson Audio-Visual Database of Emotional Speech and Song). This dataset contains emotional speech samples from actors performing scripted lines in various emotional states. The RAVDESS dataset includes a variety of emotions such as anger, happiness, sadness, surprise, and more.


## Results
The model achieves an accuracy of 85% on the test data. The performance metrics, such as training and validation accuracy and loss, are displayed in the graphs below:
#### Training Accuracy: Shows how the model's accuracy improves with each epoch.
#### Validation Accuracy: Indicates how well the model generalizes to unseen data.
#### Training Loss: Demonstrates the decrease in error during training.
#### Validation Loss: Shows the error on the validation set


# License
This project is licensed under the MIT License.
